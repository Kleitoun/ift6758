---
layout: post
title: Milestone 2
---
<div style="text-align: justify">
This post focuses on discussions based on the problem statements mentioned in Milestone-2 of the Project for IFT 6758.
    </div>
    
# Question 2

### 2.1 

![image](./figures/milestone2/q_2_1_goals_distance.png)
![image](./figures/milestone2/q_2_1_shots_distance.png)
<div style="text-align: center"><i>Histograms of shot counts with respect to their distance to the net(goals on top and no-goals at the bottom)</i></div><br>

<div style="text-align: justify">
    We have displayed histograms of the shot counts from the 2015/16 to the 2018/19 seasons with regards to their distance to the net. We can see that the further away players are, the less likely their shots are to convert into goals. This can be explained by the presence of defenders blocking their vision/their shots, by the reduced precision of their shot due to the distance or by the increased time the goalie has to block the shot.  
    </div>
    
![image](./figures/milestone2/q_2_1_goals_angle.png)
![image](./figures/milestone2/q_2_1_shots_angle.png)
<div style="text-align: center"><i>Histograms of shots with respect to the shot angle to the net (goals on top and no-goals at the bottom)</i></div><br>

<div style="text-align: justify">
    We have displayed histograms of the shot counts from the 2015/16 to the 2018/19 seasons with regards to the shot angle to the net. We can see that most of the goals are scored straight to the net (~0° angle to the net). However, most of the shots are also taken at 30° (+/- 5) angle, on either side of the net. Most offensive plays come from the side of the rink where players try to cut to the middle in order to get a better shot, which can explain the high proportion of shots taken at those angles.
    </div>

![image](./figures/milestone2/q_2_1_2D.png)
<div style="text-align: center"><i> 2D scatter plot and histograms of shot counts with respect to their distance (x-axis) and their angle (y-axis)</i></div><br>

<div style="text-align: justify">
    Looking at the 2D histogram, we see that the further away we are from the net the closer, the less shots are taken at a big angle. Yet, closer shots taken at almost a 90° degree angle are more likely to be converted into goals, which can seem contradictory since the attacker has less options to shoot. 
    </div>

### 2.2 
![image](./figures/milestone2/q_2_2_distance.png)
![image](./figures/milestone2/q_2_2_angle.png)
<div style="text-align: center"><i>Graphs relating goal rate and shot distance and relating goal rate and shot angle</i></div><br>

<div style="text-align: justify">
    These two plots confirm our previous observations. Most shots that convert to goals are closer to the net and/or taken at a small angle with respect to the net. We can observe that some shots convert to goals when taken at the greatest distance possible: usually, when teams are behind in score towards the end of the game, they will sub out their goalie in order to have an additional player on the rink - this gives the opportunity to the other team to shot in the empty from their zone. 
    </div>

## 2.3

![image](./figures/milestone2/q_2_3_empty_nets.png)
![image](./figures/milestone2/q_2_3_non_empty_nets.png)

<div style="text-align: justify">
    The two histograms show the proportion of goals scored on empty and non-empty nets. The clear difference that is observable is the distance at which empty nets goals are scored compared to non-empty nets goals. Most goals that are scored on non-empty goals are relatively close to the net - as discussed earlier, the presence of the goalie makes it harder to score from afar. However,  when the nets are empty, the offensive team has more options and can choose to shoot at the net from almost anywhere on the rink.
    </div>
    
    
# Question 3

## 3.1 

![image](./figures/milestone2/Q3_heat_map.png)
<div style="text-align: center"><i>Heatmap of predictions on the validation set from the logistic regression trained using only the distance feature</i></div><br>

<div style="text-align: justify">
    Is it clear from the heatmap generated that the logistic regression model didn't properly classify every datapoint. Even though the accuracy was of 90%, there aren't any shots that were classified as goals, resulting in a bad representation of the dataset. This can be explained by the high level of imbalance in our dataset - out of all the shots taken, only a small proportion of them get converted into goals.
    </div>

## 3.2 and 3.3

![image](./figures/milestone2/Q3_ROC_curve.png)
<div style="text-align: center"><i>Receiver Operating Characteristic curves of the logistic regression model trained on three different combinations of features and a random baseline</i></div><br>

![image](./figures/milestone2/Q3_Goal_Rate.png)
<div style="text-align: center"><i>Goal rate curves of the logistic regression model trained on three different combinations of features and a random baseline</i></div><br>

![image](./figures/milestone2/Q3_Cum_Goal.png)
<div style="text-align: center"><i>Cumulative proportion of goals curves of the logistic regression model trained on three different combinations of features and a random baseline</i></div><br>

![image](./figures/milestone2/Q3_Calibration_Curve.png)
<div style="text-align: center"><i>Reliability diagram of the logistic regression model trained on three different combinations of features and a random baseline</i></div><br>

<div style="text-align: justify">
    
    </div>

# Question 4
<div style="text-align: justify">
    New features have been added to our dataframe ; they are listed as below:<br>
    -game_seconds: <i>number of seconds elapsed since the beginning of the game, regardless of period</i><br>
    -empty_net: <i> 1 if net empty, 0 otherwise</i><br>
    -gwg: <i> if shot resulted in a goal, specifies if the goal was game winning or not</i><br>
    -distance_from_net: <i>distance of the shot from the net</i><br>
    -angle_from_net: <i>angle at which the shot was taken with respect to the other teams net</i><br>
    -shot_type_#TYPE: <i>one-hot encoded as 1 if from {Backhand, Deflected, Slap shot, Snap shot, Tip-in, Wrap-around, Wrist shot} and 0 otherwise</i><br>
    -previous_event_type: <i>type of the event before the shot happened</i><br>
    -previous_x_(y_)coordinates: <i>x and y-coordinates of the previous event</i><br>
    -previous_event_period: <i>period at which the previous event occured</i><br>
    -previous_attacking_team: <i>team that had possession of the puck during the previous event</i><br>
    -previous_event_game_seconds: <i>number of seconds elasped from the beginning of the game until the previous event happened, regardless of period</i><br>
    -time_since_last_event: <i>difference in game seconds between current event and previous event</i><br>
    -distance_from_last_event: <i>distance between the current event and the previous one</i><br>
    -time_since_powerplay: <i>seconds elapsed since the first powerplay (can cumulate if multiple ones happen)</i><br>
    -rebound: <i>1 if current event and previous event are both shots, 0 otherwise</i><br>
    -overtime: <i>1 if the shot was taken during overtime, 0 otherwise</i><br>
    -speed: <i>ratio of time_since_last_event with respect to distance_from_last_event</i><br>
    -power_play: <i>1 if attacking team has a players advantage, 0 otherwise</i><br>
    -penalty_kill: <i> 1 if attacking team has a players disadvantage, 0 otherwise</i><br>
    -change_in_angle: <i>if the previous was also a shot, compute the difference in the shot angle of the previous and current events</i><br>
    -home_players: <i>number of home players on the rink during that event</i><br>
    -away_players: <i>number of away players on the rink during that event</i><br>
    -5/4/3v5/4/3: <i>description of a players distribution on the rink ; either a power play if left number greater than right and penalty kill othwerise, referencing the attacking team</i><br>
    -goalie_(#GOALIE_NAME): <i>one-hot encoding of goalies ; 1 in the column corresponding to the specific goalie and 0 othwerwise</i><br>
    -attacking_team_(#TEAM_NAME): <i>one-hot encoding of the attacking team ; 1 in the column corresponding to the attacking team and 0 otherwise</i><br>
    </div>
[See experiment here.](https://www.comet.ml/kleitoun/feature-engineering-data?shareable=Ojw17s19mRnbDX1lQD5wptbbK)
     
    
# Question 5

## 5.1
**Train/Validation Setup:** 
We choose to 'stratify' over the target class variable, due to the high imbalance in it.
It is always desirable to split the dataset into train and validation sets in a way that preserves the same proportions of datapoints in each class as in the original complete dataset. This is important in order for the model to see fair number of examples from both the classes(0,1 in this case) during training.
Shuffle is used to prevent data from having all similar samples together, which can harm generalization capacity later. Random state is used to ensure reproducibility of the same aplit.

Both of these baseline models (using Logistic Regression and XGBoost) utilize same 2 features: shot distance and shot angle, while XGBoost performs better with an ROC score of ~0.71 vs ~0.69 thats' achieved using Logistic Regression. This boost can be attributed to model's inherent capability of capturing non-linear relationships.

[See experiment here](https://www.comet.ml/kleitoun/milestone-2/b2e8d79ab6fe40cb8dfef23d74c070b8)

Feature Importance plot using SHAP:
![image](./figures/milestone2/Q5_1_shap_summary_plot.png)

Following are the performance plots:
![image](./figures/milestone2/Q5_1_xgboostROC_Curve.png)
![image](./figures/milestone2/Q5_1_xgboost_Goal_Rate.png)
![image](./figures/milestone2/Q5_1_xgboost_Cum_Goal.png)
![image](./figures/milestone2/Q5_1_xgboost_Calibration_Curve.png)

## 5.2 
**Hyperparameter Tuning:** 
Tuning the hyperparameters often has a huge lmpact on optimizing the decision boundaries achieved otherwise. Given the computational limitations, its hard to try out every permutation, hence it is important to **choose** which set of hyperparameters would we want to search through.
    - **n_estimators :** XGBoost is trained sequentially, where each subsequent tree tries to learn through the errors made by the previous sequence of trees. This parameter is hence to mention how many of such tress would the algorithm need ideally.
    - **learning_rate :** Learning rate mainly controls the learning speed of the algorithm
    - **max_depth :** This determines how deep each of the trees(estimator) can go 
    - **gamma :** Gamma controls the complexity and hence aids preventing overfitting
In order to search through the ideal values for the subset of parameters, we used a Randomized Search(RandomSearchCV), that comes up with random combinations of parameters using the various options we input for each of the parameters. The set yielding the best ROC score is used to train the final model. This is said to be faster and better than Grid Search at times. Following are a few examples of such combinations:
- Set 1: {'n_estimators': 150, 'learning_rate': 0.3, 'max_depth': 18, 'gamma': 2},
![image](./figures/milestone2/hmtuning_set_1_roc.png)
- Set 2: {'n_estimators': 300, 'learning_rate': 0.1, 'max_depth': 30, 'gamma': 2},
![image](./figures/milestone2/hmtuning_set_2_roc.png)
- Set 3: {'n_estimators': 150, 'learning_rate': 0.3, 'max_depth': 18}, 
![image](./figures/milestone2/hmtuning_set_3_roc.png)
- Set 4: {'n_estimators': 300, 'learning_rate': 0.1, 'max_depth': 30, 'gamma': 2, 'min_child_weight': 0.25}
![image](./figures/milestone2/hmtuning_set_4_roc.png)

Set 4 yields the best ROC AUC score.(We do not concentate on accuracy due to the huge data imbalance)
Apart from this, a performance boost was find post adding the class weights. As setting these using 'balanced' method did not have any effect, weights were manually set according to the ratio of samples of each class in the train set, to aid a more fair training environment. (This should ideally be the same, but has been a known peculiarity for a while). 
A reasonable boost in performace as compared to the Baseline model(~0.71) was achieved, scoring(ROC AUC) ~0.761.

[See experiment here](https://www.comet.ml/kleitoun/milestone-2/e5a992ce9cbb4708928220e6f73f9263)

Following are the performance plots:
![image](./figures/milestone2/Q5_2_xgboost_ROC_Curve.png)
![image](./figures/milestone2/Q5_2_xgboost_Goal_Rate.png)
![image](./figures/milestone2/Q5_2_xgboost_Cum_Goal.png)
![image](./figures/milestone2/Q5_2_xgboost_Calibration_Curve.png)

## 5.3 
**Feature Selection:**
We use the model trained above to fetch the feature importance scores, or the features that XGBoost found impactful to make the decisions. Following is a plot depicting the same:
![image](./figures/milestone2/Q5_feature_importance.png)

We then use Recursive Feature Elimination for feature selection. This is essentially a method where we iteratively set a threshold over the feature importance scores from above, to remove one(or more) features and choose the setting that yields the best score. 
The second best alternative uses 28 out of the 31 features extracted before in part 4. 
While the best used all of the features(model with only hyperparameter tuning and class weights). The second best used 28 out of 31 features.

[See experiment here](https://www.comet.ml/kleitoun/milestone-2/5d20421b4b2f44ec89581e3b7d11e5db)

Following are the performance plots for the second best model with 28 features:
![image](./figures/milestone2/Q5_3_xgboost_ROC_Curve.png)
![image](./figures/milestone2/Q5_3_xgboost_Goal_Rate.png)
![image](./figures/milestone2/Q5_3_xgboost_Cum_Goal.png)
![image](./figures/milestone2/Q5_3_xgboost_Calibration_Curve.png)

# Question 6

## Random Forest

![image](./figures/milestone2/Q6Random Forest_ROC_Curve.png)
![image](./figures/milestone2/Q6Random Forest_Goal_Rate.png)
![image](./figures/milestone2/Q6Random Forest_Cum_Goal.png)
![image](./figures/milestone2/Q6Random Forest_Calibration_Curve.png)
[See here for the experiment of the Random Forest Model.](https://www.comet.ml/kleitoun/random-forest?shareable=JM4DUA7zcrHT3pP7MoGstDeal)

## Neural Network
Our next experiment was with Neural Networks. We decided to go with a Softmax classifier with RELU activation functions, built in pytorch. At first, we tried to include as many features as possible (such as attacking team and defending team one-hot encoded), but the models would get stuck in a local minimum and predict a goal probability of 0% for every single datapoint. We removed some of these features manually, after which our models where able to train proprely. After some hyperparameter tuning ([See here for grid search](https://www.comet.ml/kleitoun/neural-net/53b11ebc6a624822b5f4126253312851))), we settled on a 4-layer network with hidden dimensions of 16, 10 and 8 trained with Adam and a learning rate of 0.001. We retrained the model for 2000 more epochs [here](https://www.comet.ml/kleitoun/neural-net/f6b8ce4a119e4143abbc98bbbc55edd8) since it looked like it may not have finished converging during our grid search. The performance of this model fell short of our random forest experiment, achieving an ROC AUC of 0.743. The four diagnostic plots are show below:
![image](./figures/milestone2/Q6_neural_net_ROC_Curve.png)
![image](./figures/milestone2/Q6_neural_net_Goal_Rate.png)
![image](./figures/milestone2/Q6_neural_net_Cum_Goal.png)
![image](./figures/milestone2/Q6_neural_net_Calibration_Curve.png)

## Considerations related to training/validation split
So far, our experiments had been performed on train and validation set that were generated randomly from the 2015/16-2018/19 data. However, this is not how our model will be tested: we will be using 2015/16-2018/19 for modeling and 2019/20 for testing. This is often how machine learning models are used in practice, where past data is used to model the future. We felt that this notion of time could have a non-negligible impact on the performance of our model. For example, changes in strategy or in how events are logged can occur over time, which could make a model trained on 2015/16-2018/19 perform poorly for the season of 2019/20. If this was the case, our experiments so far could have been overstimating the performance of our model. To estimate the impact that time could have, we trained a Neural Network with the same architecture as above, this time using 2015/16-2017/18 as our training set and 2018/19 as our validaiton set. The commet log can be found [here](https://www.comet.ml/kleitoun/neural-net/d193966f5e7d4a439c0f7153120a84cd). The results here were ecouraging: our model performed as well as in our previous experiment, as can be seen below. This increases our confidence that our best model will generalise fairly well to the 2019/20 data.
![image](./figures/milestone2/Q6_neural_net_ROC_Curve_season_split.png)
![image](./figures/milestone2/Q6_neural_net_Goal_Rate_season_split.png)
![image](./figures/milestone2/Q6_neural_net_Cum_Goal_season_split.png)
![image](./figures/milestone2/Q6_neural_net_Calibration_Curve_season_split.png)
