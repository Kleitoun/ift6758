{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install comet-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c3d2e03543a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import before anyone else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcomet_ml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/comet_ml/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_setup_comet_http_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_setup_comet_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m from ._reporting import (\n\u001b[1;32m     23\u001b[0m     \u001b[0mEXPERIMENT_CREATED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/comet_ml/_logging.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mjson_encoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNestedEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__copyright__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__cake__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcerts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m# to_native_string is unused here, but imported here for backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_internal_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_native_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/certs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpackaged\u001b[0m \u001b[0mCA\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \"\"\"\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcertifi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/certifi/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2020.12.05\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/certifi/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0m_CACERT_CTX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import before anyone else\n",
    "\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import shap\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('../ift6758/data/')\n",
    "sys.path.append('../ift6758/visualizations/')\n",
    "\n",
    "from question_5_plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_no = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'final_df.csv', index_col = None)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features from Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_type_feat = [x for x in df.columns if \"shot_type_\" in x]\n",
    "len(shot_type_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat = [\"game_seconds\", \"period\", 'x_coordinates','y_coordinates', 'shot_type', 'distance_from_net', 'angle_from_net',\n",
    "               \"previous_event_type\", 'previous_event_x_coordinates', 'previous_event_y_coordinates',\n",
    "              'time_since_last_event', 'distance_from_last_event', 'rebound', 'change_in_angle', 'speed',\n",
    "              'time_since_powerplay_started', '5v5','4v4','3v3','5v4','5v3','4v3','4v5','3v5','3v4']\n",
    "new_feat.extend(shot_type_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['season'] != 20192020]\n",
    "df_train = df_train[df_train['season_type'] == 'R']\n",
    "df_test = df[df['season'] == 20192020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Train-Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = df_train[['angle_from_net', 'distance_from_net']]\n",
    "y_1 = df_train['goal_ind']\n",
    "#y = y.astype(int)\n",
    "# (#BLOG)\n",
    "X_train_1, X_valid_1, y_train_1, y_valid_1 = train_test_split(X_1,y_1,test_size=0.15,random_state=10, stratify=y_1, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Train-Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_features = df_train[new_feat].select_dtypes('object').columns\n",
    "obj_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df.drop(columns = ['shot_type'], inplace = True)\n",
    "    #convert string values to numerical values\n",
    "    le = LabelEncoder()\n",
    "    df[\"previous_event_type\"] = le.fit_transform(df[\"previous_event_type\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = df_train[new_feat]\n",
    "y_2 = df_train['goal_ind']\n",
    "\n",
    "X_2 = preprocess(X_2)\n",
    "\n",
    "print(X_2.columns)\n",
    "#y = y.astype(int)\n",
    "X_train_2, X_valid_2, y_train_2, y_valid_2 = train_test_split(X_2,y_2,test_size=0.15,random_state=10, stratify=y_2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Baseline XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Validation Setup:\n",
    "We choose to 'stratify' over the target class variable, due to the high imbalance in it.\n",
    "It is always desirable to split the dataset into train and validation sets in a way that preserves the same proportions of datapoints in each class as in the original complete dataset. This is important in order for the model to see fair number of examples from both the classes(0,1 in this case) during training.\n",
    "Shuffle is used to prevent data from having all similar samples together, which can harm generalization capacity later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1 = Experiment(api_key = os.environ.get('COMET_API_KEY'), project_name = \"milestone-2\", workspace=\"kleitoun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1 = Experiment(api_key = os.environ.get('COMET_API_KEY'), project_name = \"milestone-2\", workspace=\"kleitoun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = XGBClassifier()\n",
    " \n",
    "# fit the model with the training data\n",
    "model_1.fit(X_train_1,y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_1 = model_1.predict(X_valid_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Importance with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_X_train_1 = X_train_1.rename(columns = {\"angle_from_net\": \"Shot Angle\", \"distance_from_net\": \"Shot Distance\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model_1)\n",
    "shap_values = explainer(plot_X_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - XGBoost Baseline (5.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model file, if pretrained\n",
    "\n",
    "# model_1 = pickle.load(open(\"XGBoost_Baseline_model.pickle\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USAGE ####\n",
    "model_name_1 = '_XGBoost_Baseline'\n",
    "\n",
    "perf_eval = Performance_Eval(model_1,model_name_1, X_train_1, y_train_1, X_valid_1, y_valid_1, question_no = question_no)\n",
    "roc = perf_eval.get_roc_auc_plot()\n",
    "grp = perf_eval.get_goal_rate_plot()\n",
    "crp = perf_eval.get_cum_rate_plot()\n",
    "cp = perf_eval.get_calibration_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_1.log_image(\"Q5_ROC.Curve.png\")\n",
    "# experiment_1.log_image(\"Q5_Goal_Rate.png\")\n",
    "# experiment_1.log_image(\"Q5_Cum_Goal.png\")\n",
    "# experiment_1.log_image(\"Q5_Calibration_Curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Metrics, Model and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1.log_figure(figure = roc, figure_name=f'Q{question_no}_{model_name_1}_ROC_curve.png')\n",
    "experiment_1.log_figure(figure = roc, figure_name=f'Q{question_no}_{model_name_1}_Goal_Rate.png')\n",
    "experiment_1.log_figure(figure = roc, figure_name=f'Q{question_no}_{model_name_1}_Cum_Goal.png')\n",
    "experiment_1.log_figure(figure = roc, figure_name=f'Q{question_no}_{model_name_1}_Calibration_Curve.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_1 = model_1.predict_proba(X_valid_1)[:,1]\n",
    "roc_auc =  roc_auc_score(y_valid_1,y_pred_proba_1)\n",
    "y_pred_1 = model_1.predict(X_valid_1)\n",
    "accuracy =  accuracy_score(y_valid_1, y_pred_1)\n",
    "report = classification_report(y_valid_1, y_pred_1, output_dict=True)\n",
    "\n",
    "print(roc_auc, accuracy)\n",
    "report\n",
    "#result: 0.7163373991488216 0.9062212030945288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"roc_auc\": roc_auc,\n",
    "    \"accuracy\": accuracy,\n",
    "    \"classification report\": report\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_1, open(\"XGBoost_Baseline_model.pickle\", 'wb'))\n",
    "t = pickle.load(open(\"XGBoost_Baseline_model.pickle\", 'rb'))\n",
    "\n",
    "experiment_1.log_model(name = \"XGBoost_Baseline_model\", file_or_folder = \"XGBoost_Baseline_model.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1.url\n",
    "#'https://www.comet.ml/kleitoun/milestone-2/b2e8d79ab6fe40cb8dfef23d74c070b8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_2 = Experiment(api_key = os.environ.get('COMET_API_KEY'), \n",
    "                        project_name = \"milestone-2\", \n",
    "                        workspace=\"kleitoun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove NaNs and Infs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_df(df):\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = impute_df(X_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a vanilla XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## WITH ONLY Q4 features\n",
    "model_2 = XGBClassifier()\n",
    " \n",
    "# fit the model with the training data\n",
    "model_2.fit(X_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_2 = model_2.predict(X_valid_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_2 = model_2.predict_proba(X_valid_2)[:,1]\n",
    "roc_auc =  roc_auc_score(y_valid_2,y_pred_proba_2)\n",
    "y_pred_2 = model_2.predict(X_valid_2)\n",
    "accuracy =  accuracy_score(y_valid_2, y_pred_2)\n",
    "report = classification_report(y_valid_2, y_pred_2, output_dict=True)\n",
    "\n",
    "print(roc_auc, accuracy)\n",
    "\n",
    "# result: 0.7653420509819099 0.9046782247176564"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-paramter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "    'n_estimators': [150, 300,500],\n",
    "    'learning_rate': [0.3, 0.2, 0.1],\n",
    "    'max_depth': list(range(16,30,2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# folds = 3\n",
    "# param_comb = 5\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "# # random search is said to be faster and more efficient at times\n",
    "# random_search = RandomizedSearchCV(model_2, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train_2,y_train_2), verbose=3, random_state=1001 )\n",
    "# # random_search = GridSearchCV(model_2, params, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train_2,y_train_2), verbose=3)\n",
    "\n",
    "\n",
    "# random_search.fit(X_train_2,y_train_2)\n",
    "\n",
    "# hp_params = random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_params = {'n_estimators': 150,\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 18}\n",
    "model_2_hmtuned = XGBClassifier(**hp_params)\n",
    "model_2_hmtuned.fit(X_train_2, y_train_2)\n",
    "\n",
    "# model_2_hmtuned = model_2_hmtuned.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_2 = model_2_hmtuned.predict_proba(X_valid_2)[:,1]\n",
    "roc_auc =  roc_auc_score(y_valid_2,y_pred_proba_2)\n",
    "y_pred_2 = model_2_hmtuned.predict(X_valid_2)\n",
    "accuracy =  accuracy_score(y_valid_2, y_pred_2)\n",
    "report = classification_report(y_valid_2, y_pred_2, output_dict=True)\n",
    "\n",
    "print(roc_auc, accuracy)\n",
    "\n",
    "# result: 0.7265493652260179 0.9025994899599254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_2.log_parameters(hp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Hyperparameter Tuning Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing a few combinations, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = [{'n_estimators': 150,\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 18, 'gamma': 2},\n",
    "              {'n_estimators': 300,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 30, 'gamma': 2},\n",
    "              {'n_estimators': 150,\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 18}, \n",
    "             {'n_estimators': 300,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 30, 'gamma': 2, 'min_child_weight': 0.25}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = ['parameter set 1', 'parameter set 2', 'parameter set 3']\n",
    "classifiers = [XGBClassifier(**param_list[0]),\n",
    "               XGBClassifier(**param_list[1]),\n",
    "               XGBClassifier(**param_list[2]),\n",
    "               XGBClassifier(**param_list[3])\n",
    "               ]\n",
    "\n",
    "m1 = classifiers[0].fit(X_train_2, y_train_2)\n",
    "m2 = classifiers[1].fit(X_train_2, y_train_2)\n",
    "m3 = classifiers[2].fit(X_train_2, y_train_2)\n",
    "m4 = classifiers[3].fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_2 = \"2_xgboost_set_1\"\n",
    "perf_eval = Performance_Eval(m1, model_name_2, X_train_2, y_train_2, X_valid_2, y_valid_2,question_no = question_no)\n",
    "roc_set_1 = perf_eval.get_roc_auc_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_3 = \"2_xgboost_set_2\"\n",
    "perf_eval = Performance_Eval(m2, model_name_3, X_train_2, y_train_2, X_valid_2, y_valid_2,question_no = question_no)\n",
    "roc_set_2 = perf_eval.get_roc_auc_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_4 = \"2_xgboost_set_3\"\n",
    "perf_eval = Performance_Eval(m3,model_name_4, X_train_2, y_train_2, X_valid_2, y_valid_2,question_no = question_no)\n",
    "roc_set_3 = perf_eval.get_roc_auc_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_5 = \"2_xgboost_set_4\"\n",
    "perf_eval = Performance_Eval(m4,model_name_5, X_train_2, y_train_2, X_valid_2, y_valid_2,question_no = question_no)\n",
    "roc_set_4 =  perf_eval.get_roc_auc_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = \"hmtuning\"\n",
    "experiment_2.log_figure(figure = roc_set_1, figure_name=f'Q{question_no}_{step}_ROC_curve_set_1.png')\n",
    "experiment_2.log_figure(figure = roc_set_2, figure_name=f'Q{question_no}_{step}_ROC_curve_set_2.png')\n",
    "experiment_2.log_figure(figure = roc_set_3, figure_name=f'Q{question_no}_{step}_ROC_curve_set_3.png')\n",
    "experiment_2.log_figure(figure = roc_set_4, figure_name=f'Q{question_no}_{step}_ROC_curve_set_4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result: param_list[3] turns out the most effective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To avoid heavy class imbalance problem, tuning class weights scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# assigining class weight to balanced does NOT yield any good results/makes NO difference (#BLOG)\n",
    "\n",
    "classes_weights = class_weight.compute_sample_weight(\n",
    "    # based on ratio of train set class distribution\n",
    "    class_weight={0:0.10, 1:0.90},\n",
    "    y=y_train_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_cw = XGBClassifier(**param_list[3])\n",
    "\n",
    "model_2_cw.fit(X_train_2, y_train_2, sample_weight=classes_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cw = model_2_cw.predict(X_valid_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - XGBoost post Class Weights (5.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_cw = model_2_cw.predict_proba(X_valid_2)[:,1]\n",
    "roc_auc =  roc_auc_score(y_valid_2,y_pred_proba_cw)\n",
    "y_pred_cw = model_2_cw.predict(X_valid_2)\n",
    "accuracy =  accuracy_score(y_valid_2, y_pred_cw)\n",
    "report = classification_report(y_valid_2, y_pred_cw, output_dict=True)\n",
    "\n",
    "print(roc_auc, accuracy)\n",
    "report\n",
    "\n",
    "#result: 0.7605648908916087 0.7943124102608062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"roc_auc\": roc_auc,\n",
    "    \"accuracy\": accuracy,\n",
    "    \"classification report\": report\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_2.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USAGE ####\n",
    "model_name_6 = '2_xgboost_hmtuning'\n",
    "perf_eval = Performance_Eval(model_2_cw, model_name_6, X_train_2, y_train_2, X_valid_2, y_valid_2, question_no = question_no)\n",
    "roc = perf_eval.get_roc_auc_plot()\n",
    "grp = perf_eval.get_goal_rate_plot()\n",
    "crp = perf_eval.get_cum_rate_plot()\n",
    "cr = perf_eval.get_calibration_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Model and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_2.log_figure(figure = roc, figure_name=f'Q{question_no}_{model_name_6}_ROC_curve.png')\n",
    "experiment_2.log_figure(figure = grp, figure_name=f'Q{question_no}_{model_name_6}_Goal_Rate.png')\n",
    "experiment_2.log_figure(figure = crp, figure_name=f'Q{question_no}_{model_name_6}_Cum_Goal.png')\n",
    "experiment_2.log_figure(figure = cr, figure_name=f'Q{question_no}_{model_name_6}_Calibration_Curve.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_2_cw, open(\"XGBoost_hmtuning_model_v2.pickle\", 'wb'))\n",
    "t = pickle.load(open(\"XGBoost_hmtuning_model_v2.pickle\", 'rb'))\n",
    "\n",
    "experiment_2.log_model(name = \"XGBoost_hmtuning_model_v2\", file_or_folder = \"XGBoost_hmtuning_model_v2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_2.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_2.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection (5.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_3 = Experiment(api_key = os.environ.get('COMET_API_KEY'), \n",
    "                        project_name = \"milestone-2\", \n",
    "                        workspace=\"kleitoun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important = model_2_cw.get_booster().get_score(importance_type='weight')\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "viz = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "plt = viz.nlargest(40, columns=\"score\").plot(kind='barh', figsize = (20,10)) ## plot top 40 features\n",
    "plt.figure.savefig(\"Q5_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_3.log_image(\"Q5_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To also visualize the importance of class weight and stratifying the data, \n",
    "# the difference between the feature importance graphs could be helpful(#BLOG)\n",
    "# explainer_2 = shap.Explainer(model_2_cw)\n",
    "# shap_values_2 = explainer_2(X_train_2)\n",
    "\n",
    "# shap.summary_plot(shap_values_2, X_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "#\n",
    "thresholds = np.sort(model_2_cw.feature_importances_)\n",
    "for thresh in tqdm(thresholds):\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model_2_cw, threshold=thresh, prefit=True)\n",
    "    select_X_train_2 = selection.transform(X_train_2)\n",
    "    # train model\n",
    "    selection_model = XGBClassifier(**hp_params)\n",
    "    selection_model.fit(select_X_train_2, y_train_2, sample_weight=classes_weights)\n",
    "    # eval model\n",
    "    select_X_valid_2 = selection.transform(X_valid_2)\n",
    "    y_pred = selection_model.predict(select_X_valid_2)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y_valid_2, predictions)\n",
    "    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train_2.shape[1], accuracy*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain based on the optimal accuracy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## second best thresh value is prioritzed here, as the best value allows only one feature which can make the model biased\n",
    "thresh = 0.020\n",
    "selection = SelectFromModel(model_2_cw, threshold=thresh, prefit=True)\n",
    "select_X_train_2 = selection.transform(X_train_2)\n",
    "\n",
    "selection_model = XGBClassifier(**param_list[3])\n",
    "selection_model.fit(select_X_train_2, y_train_2, sample_weight=classes_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = selection.get_support(indices=True)\n",
    "transformed_X_train_2 = X_train_2.iloc[:,cols]\n",
    "transformed_X_valid_2 = X_valid_2.iloc[:,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USAGE ####\n",
    "model_name_7 = \"3_xgboost_fs\"\n",
    "perf_eval = Performance_Eval(selection_model,model_name_7, transformed_X_train_2, y_train_2, transformed_X_valid_2, y_valid_2, question_no)\n",
    "roc = perf_eval.get_roc_auc_plot()\n",
    "grp= perf_eval.get_goal_rate_plot()\n",
    "crp = perf_eval.get_cum_rate_plot()\n",
    "cr = perf_eval.get_calibration_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Model and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_3.log_figure(figure = roc, figure_name=f'Q{question_no}_{model_name_7}_ROC_curve.png')\n",
    "experiment_3.log_figure(figure = grp, figure_name=f'Q{question_no}_{model_name_7}_Goal_Rate.png')\n",
    "experiment_3.log_figure(figure = crp, figure_name=f'Q{question_no}_{model_name_7}_Cum_Goal.png')\n",
    "experiment_3.log_figure(figure = cr, figure_name=f'Q{question_no}_{model_name_7}_Calibration_Curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(selection_model, open(\"XGBoost_feature_selection_model_v2.pickle\", 'wb'))\n",
    "t = pickle.load(open(\"XGBoost_feature_selection_model_v2.pickle\", 'rb'))\n",
    "\n",
    "experiment_3.log_model(name = \"XGBoost_feature_selection_model_v2\", file_or_folder = \"XGBoost_feature_selection_model_v2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_3.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_cw = selection_model.predict_proba(transformed_X_valid_2)[:,1]\n",
    "roc_auc =  roc_auc_score(y_valid_2,y_pred_proba_cw)\n",
    "y_pred_cw = selection_model.predict(transformed_X_valid_2)\n",
    "accuracy =  accuracy_score(y_valid_2, y_pred_cw)\n",
    "report = classification_report(y_valid_2, y_pred_cw, output_dict=True)\n",
    "\n",
    "print(roc_auc, accuracy)\n",
    "report\n",
    "\n",
    "#result: 0.7593543260298987 0.7885262413475345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"roc_auc\": roc_auc,\n",
    "    \"accuracy\": accuracy,\n",
    "    \"classification report\": report\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_3.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4 for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does not give much of an improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = impute_df(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators= 150, max_depth= 4, learning_rate= 0.2, subsample= 1.0, min_child_weight= 5, gamma= 5, colsample_bytree= 1.0)\n",
    "over = SMOTE(sampling_strategy=\"auto\")\n",
    "under = RandomUnderSampler(sampling_strategy=\"auto\")\n",
    "steps = [('over', over), ('under', under), ('model', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(pipeline, X_train_2, y_train_2, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "print('Mean ROC AUC: %.3f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter\n",
    "# summarize class distribution\n",
    "counter = Counter(y_2)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = ADASYN()\n",
    "X_train_smote, y_train_smote = oversample.fit_resample(X_2, y_2)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_train_smote)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
